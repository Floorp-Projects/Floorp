<!doctype html>
<html>

<head>
  <title>MediaStreamTrackProcessor</title>
  <link rel="help" href="https://w3c.github.io/mediacapture-insertable-streams">
</head>

<body>
  <p class="instructions">When prompted, use the accept button to give permission to use your audio and video devices.</p>
  <h1 class="instructions">Description</h1>
  <p class="instructions">This test checks that processing captured video MediaStreamTracks works as expected.</p>
  <script src=/resources/testharness.js></script>
  <script src=/resources/testharnessreport.js></script>
  <canvas id="canvas"></canvas>
  <script>
    const pixelColour = [50, 100, 150, 255];
    function makeVideoFrame(timestamp) {
      const height = 240;
      const width = 320;
      const canvas = new OffscreenCanvas(width, height);

      const ctx = canvas.getContext('2d');
      ctx.fillStyle = `rgba(${pixelColour[0]}, ${pixelColour[1]}, ${pixelColour[2]}, ${pixelColour[3]})`;
      ctx.fillRect(0, 0, width, height);

      return new VideoFrame(canvas.transferToImageBitmap(), { timestamp });
    }

    // Assert that a pixel, in RGBA bytes, approximately matches the expected value.
    function assertPixel(t, bytes, expected) {
      t.step(() => {
        assert_equals(bytes.length, expected.length, "pixel bytes not correct length")
        for (let i = 0; i < bytes.length; i++) {
          assert_less_than(bytes[i], expected[i] + 2, "Mismatched pixel");
          assert_greater_than(bytes[i], expected[i] - 2, "Mismatched pixel");
        }
      });
    }

    promise_test(async t => {
      const height = 240;
      const width = 320;
      const canvas = document.getElementById('canvas');
      canvas.width = width;
      canvas.height = height;

      const ctx = canvas.getContext('2d');
      ctx.fillStyle = `rgba(${pixelColour[0]}, ${pixelColour[1]}, ${pixelColour[2]}, ${pixelColour[3]})`;
      ctx.fillRect(0, 0, width, height);


      const stream = canvas.captureStream(10);
      assert_equals(stream.getVideoTracks().length, 1);
      const videoTrack = stream.getVideoTracks()[0];

      return new Promise(async (resolve, reject) => {
        const writableStream = new WritableStream({
          write(videoFrame) {
            t.step(() => {
              assert_true(videoFrame instanceof VideoFrame);
              assert_equals(videoFrame.codedWidth, 320);
              assert_not_equals(videoFrame.timestamp, null);
            });

            videoFrame.createImageBitmap().then(bitmap => {
              const canvas = new OffscreenCanvas(bitmap.width, bitmap.height);
              const canvasCtx = canvas.getContext('2d');
              canvasCtx.drawImage(bitmap, 0, 0);

              // Check the provided frame matches the canvas input.
              const imgData = canvasCtx.getImageData(0, 0, canvas.width, canvas.height);
              for (let i = 0; i < canvas.height; i++) {
                for (let j = 0; j < canvas.width; j++) {
                  assertPixel(t, imgData.data.slice(i * canvas.width + j * 4, i * canvas.width + j * 4 + 4), pixelColour);
                }
              }
              resolve();
            });
          },
          close() {
            fail("Closed");
          },
          abort(err) {
            fail("Sink error:", err);
          }
        });

        const videoTrackProcessor = new MediaStreamTrackProcessor(videoTrack);
        videoTrackProcessor.readable.pipeTo(writableStream);

        t.step(() => {
          assert_false(videoTrack.muted, "Video track shouldn't be muted after attaching Processing.");
        });
      });
    }, "Tests that creating a Video MediaStreamTrackProcessor works as expected");

    promise_test(async t => {
      const iAmNotATrack = "notatrack";
      assert_throws_js(TypeError, () => { new MediaStreamTrackProcessor(iAmNotATrack) });
    }, "Tests that construction of a MediaStreamTrackProcessor with an invalid track throws.");

    promise_test(async t => {
      const bufferSize = 3;
      const framesToSend = 10;

      const generatedTrack = new MediaStreamTrackGenerator("video");
      const frameWriter = generatedTrack.writable.getWriter();

      const trackProcessor = new MediaStreamTrackProcessor(generatedTrack, bufferSize);
      const reader = trackProcessor.readable.getReader();

      // Enqueue many frames one after the other.
      await frameWriter.ready;
      for (let i = 0; i < framesToSend; i++) {
        await frameWriter.write(makeVideoFrame(i))
      }

      // Our reader should only be provided with the |bufferSize| latest frames.
      const framesSeen = 0;
      for (let i = 0; i < bufferSize; i++) {
        const videoFrame = (await reader.read()).value;
        t.step(() => {
          assert_true(videoFrame instanceof VideoFrame, "Invalid type of videoFrame");
          assert_greater_than_equal(videoFrame.timestamp, framesToSend - bufferSize, "Received buffered frame which should have been dropped");
        });
      }
      return Promise.resolve();
    }, "Tests that only the maxBufferSize most recent frames are stored");
  </script>
</body>

</html>
